{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6b27ae-4bbd-4dff-a8a7-332ec166218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731b5500-c6b9-48b9-a8e4-8f0899216fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_idx_to_state(idx, discrete_d, discrete_v, discrete_phi_t):\n",
    "    '''\n",
    "    map the index(row number) of the Q-table to a state\n",
    "    '''\n",
    "    len_1 = len(discrete_d) #the length of the list containing discretized distances\n",
    "    len_2 = len(discrete_v) #the length of the list containing discretized velocities\n",
    "    len_3 = len(discrete_phi_t) #the length of the list containing discretized pair of (phi, t_phi)\n",
    "    \n",
    "    idx_1 = int(np.floor(idx/(len_2*len_3))) #calculate the index of the distance in discrete_d\n",
    "    distance = discrete_d[idx_1] #the value of distance in the mapped state\n",
    "    \n",
    "    idx_2 = int(np.floor((idx - idx_1*len_2*len_3)/len_3)) #compute the index of the mapped velocity in discrete_v\n",
    "    velocity = discrete_v[idx_2] #mapped velocity\n",
    "    \n",
    "    idx_3 = idx - idx_1*len_2*len_3 - idx_2*len_3 #the index of the desired (phi, t_phi) in discrete_phi_t\n",
    "\n",
    "    phi = discrete_phi_t[idx_3].flatten()[0] #mapped phi\n",
    "    t_phi = discrete_phi_t[idx_3].flatten()[1] #mapped t_phi\n",
    "    \n",
    "    return (distance, velocity, phi, t_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825b828f-e2bb-4e70-9128-474bc920360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_state_to_idx(state, discrete_d, discrete_v, discrete_phi_t, T_Y, T_R):\n",
    "    '''\n",
    "    map the state to discretized state and return the index(row number) of that discretized state in Q-table\n",
    "    '''\n",
    "    distance = state[0]\n",
    "    velocity = state[1]\n",
    "    phi = state[2]\n",
    "    t_phi = state[3]\n",
    "    \n",
    "    #find the closest discrete distance to the distance of the given state\n",
    "    distance_diff = np.asarray([(distance - d)**2 for d in discrete_d]) \n",
    "    distance_idx = np.argmin(distance_diff)\n",
    "    \n",
    "    #find the closest discrete velocity to the velocity of the given state\n",
    "    velocity_diff = np.asarray([(velocity - v)**2 for v in discrete_v])\n",
    "    velocity_idx = np.argmin(velocity_diff)\n",
    "    \n",
    "    #find the closest discrete (phi, t_phi) to the (phi_t_phi) of the given state\n",
    "    t_phi_candidates = np.asarray([int(x[1]) for x in discrete_phi_t if x[0]==phi])\n",
    "    t_phi_diff = np.asarray([(t_phi - t)**2 for t in t_phi_candidates])\n",
    "    t_phi_idx = np.argmin(t_phi_diff)\n",
    "    if phi=='Y':\n",
    "        phi_t_idx = t_phi_idx\n",
    "    elif phi=='R':\n",
    "        phi_t_idx = t_phi_idx + (T_Y+1)\n",
    "    elif phi=='G':\n",
    "        phi_t_idx = t_phi_idx + (T_Y+T_R+2)\n",
    "    \n",
    "    #now we can compute the index(row number) of the discretized version of the given state in the Q-table\n",
    "    idx = distance_idx*len(discrete_v)*len(discrete_phi_t) + velocity_idx*len(discrete_phi_t) + phi_t_idx\n",
    "    \n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5007c487-1b1d-4285-aeba-2fd9341c6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(Q_row, actions, epsilon):\n",
    "    '''\n",
    "    returns the action in a state, based on the epsilon-greedy method.\n",
    "    Q-row is the row of the Q-table corresponding to a state, containing the Q-values of the action space for that state\n",
    "    '''\n",
    "    rand = np.random.uniform(0,1,1) #a rondom number to be compared with epsilon\n",
    "    if rand>epsilon:#greedy scenario\n",
    "        candidates = [i for i, x in enumerate(Q_row.flatten()) if x == np.max(Q_row.flatten())] #select the actions with the greatest Q-values\n",
    "        action_idx = random.choice(candidates)\n",
    "        \n",
    "    else:#random scenario\n",
    "        action_idx = np.random.randint(0,len(actions)) #randomly select an action \n",
    "        \n",
    "    action = actions[action_idx]\n",
    "    \n",
    "    return action, action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3ca5f5-4266-4d5e-9dad-745d7eb73e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(state, action, delta_t, phi_to_T_dict, phi_to_next_phi_dict, std_d, std_v, std_t_phi):\n",
    "    '''\n",
    "    in this function the state goes through the state proces with the given action\n",
    "    '''\n",
    "    distance = state[0] #distance of the state\n",
    "    velocity = state[1] #velocity of the state\n",
    "    phi = state[2] #phase of the state\n",
    "    t_phi = state[3] #t_phi of the state\n",
    "    \n",
    "    distance = distance - velocity*delta_t - 0.5*action*(delta_t**2) + np.random.normal(0, std_d, 1) #update the distance of the state based on the accelaration and state process noise\n",
    "    velocity = velocity + action*delta_t + np.random.normal(0, std_v, 1) #update the velocity of the state based on the accelaration and state process noise\n",
    "    \n",
    "    #update the phi and t_phi of the state based on the accelaration and state process noise\n",
    "    if ((int(t_phi) + delta_t) <= (phi_to_T_dict[phi] + np.random.normal(0, std_t_phi, 1))):\n",
    "        phi = phi\n",
    "        t_phi = int(t_phi) + delta_t\n",
    "\n",
    "    else:\n",
    "        phi = phi_to_next_phi_dict[phi]\n",
    "        t_phi = 0\n",
    "        \n",
    "    return (distance[0], velocity[0], phi, t_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35855d4-c2c1-4532-a656-fa4043f85441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(distance, velocity, phi, t_phi, v_max, T_Y):\n",
    "    '''\n",
    "    reward function for the Q-learning\n",
    "    '''\n",
    "    reward = -1 #reward for each taken action\n",
    "    \n",
    "    if ((velocity < 0) or (velocity > v_max)): #the reward for the case that the velocity gets less than zero or greater than maximum allowed velocity\n",
    "        reward = reward - 100\n",
    "    if distance<=0: #passing the intersection\n",
    "        if (phi=='G') or (phi=='Y'): #if the vehicle passes the intersection safely\n",
    "            if velocity>0:\n",
    "                reward = reward + 100\n",
    "        else: #if the vehilce violates the red traffic light\n",
    "            reward = reward - 100\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1a0064-8799-441e-a88a-1857c1cf0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_function(reward, discount_factor, Q_row, q): \n",
    "    return reward + discount_factor*np.max(Q_row.flatten()) - q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebfeab-c7cc-4b0a-9b13-87c89a68dad2",
   "metadata": {},
   "source": [
    "Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d8ba1f-ff41-4f6f-b543-a243fa8a654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_Y = 5 #total timing of yellow phase\n",
    "T_G = 10 #total timing of green phase\n",
    "T_R = 10 #total timing of red phase\n",
    "total_T = T_Y + T_G + T_R\n",
    "\n",
    "delta_t = 1\n",
    "\n",
    "phi_to_T_dict = {'Y': T_Y, 'G': T_G, 'R': T_R} #the dictionary to map each phase to its timing\n",
    "phi_to_next_phi_dict = {'G':'Y', 'Y':'R', 'R':'G'} #the dictionary to map each phase to its next occuring phase\n",
    "std_d=2 #standard deviation of state process, for distance\n",
    "std_v=2  #standard deviation of state process, for velocity\n",
    "std_t_phi=2 #standard deviation of state process, for timing\n",
    "\n",
    "trials = 100000 #number of loops for training of the Q-learning\n",
    "episodes = 100 #the maximum length of each trajectory \n",
    "\n",
    "learning_rate = 0.1 #learning rate of the training phase in Q-learning\n",
    "discount_factor = 0.9 #discoundt factor in Q-learning\n",
    "epsilon = 0.1 #epsilon in training phase of Q-learning\n",
    "\n",
    "actions = np.asarray([-3, -2, -1, 0, 1, 2, 3]) #action space\n",
    "discrete_d = np.arange(-8, 121, 8) #the list of distances to discretize the distances\n",
    "v_max=15 #maximum allowed velocity\n",
    "discrete_v = np.arange(0, v_max+1, 1) #the valuse to discretize velocity\n",
    "\n",
    "#discretizing values for the pair of(phi, t_phi)\n",
    "discrete_phi_t = np.asarray([('Y', 0), ('Y', 1), ('Y', 2), ('Y', 3), ('Y', 4), ('Y', 5)\\\n",
    "                            , ('R', 0), ('R', 1), ('R', 2), ('R', 3), ('R', 4), ('R', 5), ('R', 6), ('R', 7), ('R', 8), ('R', 9), ('R', 10)\\\n",
    "                            , ('G', 0), ('G', 1), ('G', 2), ('G', 3), ('G', 4), ('G', 5), ('G', 6), ('G', 7), ('G', 8), ('G', 9), ('G', 10)])\n",
    "Q_table = np.zeros((len(discrete_d)*len(discrete_v)*len(discrete_phi_t), len(actions))) #initialize the Q-table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105be0c-c539-4611-a60f-9f2944f56379",
   "metadata": {},
   "source": [
    "Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c412edf-f623-4625-8714-c53dec9b4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for trial in range(trials):\n",
    "    idx_i = np.random.randint(0, Q_table.shape[0], 1) #randomly select an index(row number) of Q-table\n",
    "    state = map_idx_to_state(idx_i, discrete_d, discrete_v, discrete_phi_t) #map the index to a state \n",
    "    mapped_state = map_idx_to_state(idx_i, discrete_d, discrete_v, discrete_phi_t) #map the index to a discretized state\n",
    "    distance = mapped_state[0] \n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        if distance<=0: # if the vehicle passed the intersection break the loop and go to the next trial\n",
    "            break\n",
    "            \n",
    "        action, action_idx = action_selection(Q_table[idx_i], actions, epsilon) #select the action based on the Q-table and with epsilon-greedy approach\n",
    "        q = Q_table[idx_i, action_idx] #the q-value related to the (state, action)\n",
    "        \n",
    "        state_new = update_state(mapped_state, action, delta_t, phi_to_T_dict, phi_to_next_phi_dict, std_d, std_v, std_t_phi) #the state goes through the state process with action\n",
    "        idx_i_new = map_state_to_idx(state_new, discrete_d, discrete_v, discrete_phi_t, T_Y, T_R) #index of the updated state in the Q-table\n",
    "        mapped_state_new = map_idx_to_state(idx_i_new, discrete_d, discrete_v, discrete_phi_t) #discretized state corresponding to the index\n",
    "        distance = mapped_state_new[0]\n",
    "        velocity = mapped_state_new[1]\n",
    "        phi = mapped_state_new[2]\n",
    "        t_phi = mapped_state_new[3]\n",
    "        \n",
    "        reward = reward_function(distance, velocity, phi, t_phi, v_max, T_Y) #reward for getting into the new state\n",
    "        \n",
    "        td = TD_function(reward, discount_factor, Q_table[idx_i_new], q)\n",
    "        \n",
    "        q = q + learning_rate*td #updating q-value\n",
    "        Q_table[idx_i,action_idx] = q #updating the q-table with the q-value\n",
    "        \n",
    "        idx_i = idx_i_new #updating the index\n",
    "        state = state_new #updating the state\n",
    "        mapped_state=mapped_state_new\n",
    "\n",
    "\n",
    "with open('Q_table.npy','wb') as f:\n",
    "    np.save(f, Q_table) #save the Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3161b-7656-4e5b-942c-39cc05a879b1",
   "metadata": {},
   "source": [
    "Q-Table Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8dfdc5b-8334-4e4b-9230-a11676f697ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of non-zero elements in Qtable:0.8177333433373349\n"
     ]
    }
   ],
   "source": [
    "#loading Q-table\n",
    "with open('Q_table.npy','rb') as f:\n",
    "    Q_table = np.load(f)\n",
    "print(f'Percentage of non-zero elements in Qtable:{len(Q_table[Q_table!=0])/(Q_table.shape[0]*Q_table.shape[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbf243-eb00-4932-84d4-f21bd60f2adb",
   "metadata": {},
   "source": [
    "Test Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30194204-7ef2-443e-a24e-dfc04819898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual state: (120, 15, 'G', 9)\n",
      "mapped state: (120, 15, 'G', '9')\n",
      "[-0.1         1.92034057  0.         -0.1        -0.11951307 -0.10981\n",
      " -0.1194409 ]\n",
      "action: -2\n",
      "\n",
      "actual state: (106.0, 13.0, 'G', 10)\n",
      "mapped state: (104, 13, 'G', '10')\n",
      "[ 0.46038945  0.44817432 -0.10981    -0.1        -0.20791    -0.109\n",
      "  6.95878412]\n",
      "action: 3\n",
      "\n",
      "actual state: (89.5, 16.0, 'Y', 0)\n",
      "mapped state: (88, 15, 'Y', '0')\n",
      "[10.07807667  0.09290867 -0.10985301 -0.3997261  -0.14808479 -0.26922289\n",
      " -0.28733912]\n",
      "action: -3\n",
      "\n",
      "actual state: (74.5, 12.0, 'Y', 1)\n",
      "mapped state: (72, 12, 'Y', '1')\n",
      "[12.440342    0.71772892  0.6393232   0.85084875  0.06995606  0.52045491\n",
      "  0.79337693]\n",
      "action: -3\n",
      "\n",
      "actual state: (61.5, 9.0, 'Y', 2)\n",
      "mapped state: (64, 9, 'Y', '2')\n",
      "[ 1.69839126  4.71842747  1.83791077 14.93566279  0.81033627  0.45652254\n",
      " -0.2881    ]\n",
      "action: 0\n",
      "\n",
      "actual state: (55.0, 9.0, 'Y', 3)\n",
      "mapped state: (56, 9, 'Y', '3')\n",
      "[ 1.30192779 17.70633736  4.64967925 -0.297739    0.57174352  2.51583259\n",
      "  2.99491857]\n",
      "action: -2\n",
      "\n",
      "actual state: (48.0, 7.0, 'Y', 4)\n",
      "mapped state: (48, 7, 'Y', '4')\n",
      "[ 3.5262995  20.78481948  5.28266988  8.32002059  7.77294331  4.8854613\n",
      "  3.2692021 ]\n",
      "action: -2\n",
      "\n",
      "actual state: (42.0, 5.0, 'Y', 5)\n",
      "mapped state: (40, 5, 'Y', '5')\n",
      "[ 7.35853053  3.54107378  6.21503704  4.16160966  8.0837175   9.30473854\n",
      " 24.20535497]\n",
      "action: 3\n",
      "\n",
      "actual state: (33.5, 8.0, 'R', 0)\n",
      "mapped state: (32, 8, 'R', '0')\n",
      "[ 5.12760277  2.15249492 12.91122802 28.00594997  8.94123946 20.62553607\n",
      "  8.79219133]\n",
      "action: 0\n",
      "\n",
      "actual state: (24.0, 8.0, 'R', 1)\n",
      "mapped state: (24, 8, 'R', '1')\n",
      "[ 7.05445659 32.2288333  13.03005642  6.19223125 -4.94905849 -4.73213793\n",
      " -6.454696  ]\n",
      "action: -2\n",
      "\n",
      "actual state: (17.0, 6.0, 'R', 2)\n",
      "mapped state: (16, 6, 'R', '2')\n",
      "[ 25.84587755  11.3990454   36.92092589 -18.37954861 -19.28740396\n",
      " -40.82132035 -35.50504691]\n",
      "action: -1\n",
      "\n",
      "actual state: (10.5, 5.0, 'R', 3)\n",
      "mapped state: (8, 5, 'R', '3')\n",
      "[ 42.1343621  -85.84044184 -85.84044184 -85.84044184 -92.0484325\n",
      " -95.12687656 -87.35639765]\n",
      "action: -3\n",
      "\n",
      "actual state: (4.5, 2.0, 'R', 4)\n",
      "mapped state: (8, 2, 'R', '4')\n",
      "[47.08155601 46.17647902 47.42089666 47.58752592 46.04131568 47.829294\n",
      " 47.927069  ]\n",
      "action: 3\n",
      "\n",
      "actual state: (4.5, 5.0, 'R', 5)\n",
      "mapped state: (8, 5, 'R', '5')\n",
      "[  54.36341    -100.99050134 -100.98944594 -100.98697029 -100.96636805\n",
      " -100.95847907 -100.98391394]\n",
      "action: -3\n",
      "\n",
      "actual state: (4.5, 2.0, 'R', 6)\n",
      "mapped state: (8, 2, 'R', '6')\n",
      "[61.50514879 61.51132187 61.48453986 61.51077055 61.5149     61.50917568\n",
      " 61.4230376 ]\n",
      "action: 1\n",
      "\n",
      "actual state: (5.5, 3.0, 'R', 7)\n",
      "mapped state: (8, 3, 'R', '7')\n",
      "[  69.45913225   69.42692825   69.45950798   69.45297145   69.461\n",
      " -100.99632003 -100.99230609]\n",
      "action: 1\n",
      "\n",
      "actual state: (4.5, 4.0, 'R', 8)\n",
      "mapped state: (8, 4, 'R', '8')\n",
      "[  78.24815168   69.43600061   78.29       -100.9985743  -100.98552255\n",
      " -100.99823988 -100.92968394]\n",
      "action: -1\n",
      "\n",
      "actual state: (4.5, 3.0, 'R', 9)\n",
      "mapped state: (8, 3, 'R', '9')\n",
      "[  61.51489945   61.51488762   78.28999127   88.09999962   88.1\n",
      " -100.99999877 -100.99999928]\n",
      "action: 1\n",
      "\n",
      "actual state: (4.5, 4.0, 'R', 10)\n",
      "mapped state: (8, 4, 'R', '10')\n",
      "[69.46099667 69.46099858 69.46099349 99.         98.99999958 98.99999892\n",
      " 98.99999988]\n",
      "action: 0\n",
      "\n",
      "(0, 4, 'G', '0')\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "f = open('./q_learning_results.txt', 'w')\n",
    "state = (120, 15, 'G', 9)\n",
    "idx_i = map_state_to_idx(state, discrete_d, discrete_v, discrete_phi_t, T_Y, T_R)\n",
    "mapped_state = map_idx_to_state(idx_i, discrete_d, discrete_v, discrete_phi_t)\n",
    "distance = mapped_state[0]\n",
    "\n",
    "while (distance>0):\n",
    "    f.write(f'actual state: {[str(i) for i in state]}\\n')\n",
    "    print(f'actual state: {state}')\n",
    "    f.write(f'mapped state: {[str(i) for i in mapped_state]}\\n')\n",
    "    print(f'mapped state: {mapped_state}')\n",
    "    \n",
    "    action, action_idx = action_selection(Q_table[idx_i], actions, 0)\n",
    "    \n",
    "    f.write(f'Q values for actions of this state:{[str(i) for i in Q_table[idx_i]]}\\n')\n",
    "    print(Q_table[idx_i])\n",
    "    f.write(f'action: {action}\\n\\n')\n",
    "    print(f'action: {action}\\n')\n",
    "    \n",
    "    state_new = update_state(mapped_state, action, delta_t, phi_to_T_dict, phi_to_next_phi_dict, std_d, std_v, std_t_phi)\n",
    "    idx_i_new = map_state_to_idx(state_new, discrete_d, discrete_v, discrete_phi_t, T_Y, T_R)\n",
    "    mapped_state_new = map_idx_to_state(idx_i_new, discrete_d, discrete_v, discrete_phi_t)\n",
    "    distance = mapped_state_new[0]\n",
    "    \n",
    "    state=state_new\n",
    "    idx_i=idx_i_new\n",
    "    mapped_state=mapped_state_new\n",
    "    \n",
    "print(mapped_state)\n",
    "print(Q_table[idx_i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64c543-e0ba-49a4-844a-18f1852e5bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
